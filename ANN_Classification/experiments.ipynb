{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:There is a known slowdown when using v2.11+ Keras optimizers on M1/M2 Macs. Falling back to the legacy Keras optimizer, i.e., `tf.keras.optimizers.legacy.Adam`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "数据预处理完成，训练集和测试集已准备好。\n",
      "训练集包含 8000 个样本，测试集包含 2000 个样本。\n",
      "特征维度为 12。\n",
      "目标变量的类别数为 2。\n",
      "目标变量的类别分布为：\n",
      "Exited\n",
      "0    7963\n",
      "1    2037\n",
      "Name: count, dtype: int64\n",
      "训练集目标变量的类别分布为：\n",
      "Exited\n",
      "0    6356\n",
      "1    1644\n",
      "Name: count, dtype: int64\n",
      "测试集目标变量的类别分布为：\n",
      "Exited\n",
      "0    1607\n",
      "1     393\n",
      "Name: count, dtype: int64\n",
      "数据预处理完成。\n",
      "数据已保存为 pickle 文件。\n",
      "label_encoder_gender.pkl\n",
      "onehot_encoder_geo.pkl\n",
      "scaler.pkl\n",
      "train_test_data.pkl\n",
      "Model: \"sequential_3\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_9 (Dense)             (None, 64)                832       \n",
      "                                                                 \n",
      " dense_10 (Dense)            (None, 32)                2080      \n",
      "                                                                 \n",
      " dense_11 (Dense)            (None, 1)                 33        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 2,945\n",
      "Trainable params: 2,945\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:There is a known slowdown when using v2.11+ Keras optimizers on M1/M2 Macs. Falling back to the legacy Keras optimizer, i.e., `tf.keras.optimizers.legacy.Adam`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "225/225 [==============================] - 1s 4ms/step - loss: 0.5243 - accuracy: 0.7872 - val_loss: 0.6686 - val_accuracy: 0.7987\n",
      "Epoch 2/10\n",
      "225/225 [==============================] - 1s 3ms/step - loss: 1.6774 - accuracy: 0.7308 - val_loss: 0.8651 - val_accuracy: 0.8250\n",
      "Epoch 3/10\n",
      "225/225 [==============================] - 1s 4ms/step - loss: 3.3052 - accuracy: 0.7296 - val_loss: 8.1319 - val_accuracy: 0.6875\n",
      "Epoch 4/10\n",
      "225/225 [==============================] - 1s 3ms/step - loss: 5.9580 - accuracy: 0.7207 - val_loss: 17.4836 - val_accuracy: 0.6525\n",
      "Epoch 5/10\n",
      "225/225 [==============================] - 1s 3ms/step - loss: 11.1274 - accuracy: 0.7235 - val_loss: 4.9835 - val_accuracy: 0.8138\n",
      "Epoch 6/10\n",
      "225/225 [==============================] - 1s 3ms/step - loss: 12.9238 - accuracy: 0.7199 - val_loss: 8.1588 - val_accuracy: 0.8075\n",
      "Epoch 7/10\n",
      "225/225 [==============================] - 1s 3ms/step - loss: 20.9674 - accuracy: 0.7233 - val_loss: 51.4670 - val_accuracy: 0.6625\n",
      "Epoch 8/10\n",
      "225/225 [==============================] - 1s 3ms/step - loss: 25.3322 - accuracy: 0.7281 - val_loss: 31.8685 - val_accuracy: 0.7350\n",
      "Epoch 9/10\n",
      "225/225 [==============================] - 1s 3ms/step - loss: 24.7374 - accuracy: 0.7260 - val_loss: 28.1763 - val_accuracy: 0.6737\n",
      "Epoch 10/10\n",
      "225/225 [==============================] - 1s 3ms/step - loss: 33.6974 - accuracy: 0.7199 - val_loss: 22.1783 - val_accuracy: 0.7812\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:There is a known slowdown when using v2.11+ Keras optimizers on M1/M2 Macs. Falling back to the legacy Keras optimizer, i.e., `tf.keras.optimizers.legacy.Adam`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "225/225 [==============================] - 1s 4ms/step - loss: 0.5390 - accuracy: 0.7819 - val_loss: 1.0183 - val_accuracy: 0.7175\n",
      "Epoch 2/10\n",
      "225/225 [==============================] - 1s 3ms/step - loss: 1.3440 - accuracy: 0.7343 - val_loss: 2.5349 - val_accuracy: 0.6275\n",
      "Epoch 3/10\n",
      "225/225 [==============================] - 1s 3ms/step - loss: 2.9123 - accuracy: 0.7236 - val_loss: 1.4341 - val_accuracy: 0.8075\n",
      "Epoch 4/10\n",
      "225/225 [==============================] - 1s 3ms/step - loss: 4.8080 - accuracy: 0.7185 - val_loss: 2.5411 - val_accuracy: 0.8250\n",
      "Epoch 5/10\n",
      "225/225 [==============================] - 1s 4ms/step - loss: 6.1578 - accuracy: 0.7264 - val_loss: 3.5173 - val_accuracy: 0.8000\n",
      "Epoch 6/10\n",
      "225/225 [==============================] - 1s 4ms/step - loss: 9.6924 - accuracy: 0.7225 - val_loss: 6.4210 - val_accuracy: 0.7937\n",
      "Epoch 7/10\n",
      "225/225 [==============================] - 1s 3ms/step - loss: 14.2347 - accuracy: 0.7226 - val_loss: 23.6570 - val_accuracy: 0.6137\n",
      "Epoch 8/10\n",
      "225/225 [==============================] - 1s 3ms/step - loss: 20.9928 - accuracy: 0.7307 - val_loss: 45.0862 - val_accuracy: 0.6363\n",
      "Epoch 9/10\n",
      "225/225 [==============================] - 1s 3ms/step - loss: 21.8880 - accuracy: 0.7319 - val_loss: 36.0630 - val_accuracy: 0.7050\n",
      "Epoch 10/10\n",
      "225/225 [==============================] - 1s 3ms/step - loss: 19.9298 - accuracy: 0.7382 - val_loss: 28.2529 - val_accuracy: 0.7000\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 27.9520 - accuracy: 0.6860\n",
      "Loss: 27.951988220214844\n",
      "Accuracy: 0.6859999895095825\n",
      "The tensorboard extension is already loaded. To reload it, use:\n",
      "  %reload_ext tensorboard\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Reusing TensorBoard on port 6006 (pid 86508), started 0:01:27 ago. (Use '!kill 86508' to kill it.)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "      <iframe id=\"tensorboard-frame-d9a076df6335cfc9\" width=\"100%\" height=\"800\" frameborder=\"0\">\n",
       "      </iframe>\n",
       "      <script>\n",
       "        (function() {\n",
       "          const frame = document.getElementById(\"tensorboard-frame-d9a076df6335cfc9\");\n",
       "          const url = new URL(\"http://localhost\");\n",
       "          const port = 6006;\n",
       "          if (port) {\n",
       "            url.port = port;\n",
       "          }\n",
       "          frame.src = url;\n",
       "        })();\n",
       "      </script>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import tensorflow as tf\n",
    "import datetime\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder, OneHotEncoder\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.callbacks import EarlyStopping, TensorBoard\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "def load_data(file_path):\n",
    "    try:\n",
    "        data = pd.read_csv(file_path)\n",
    "        return data\n",
    "    except FileNotFoundError:\n",
    "        print(f\"文件 {file_path} 未找到，请检查文件路径。\")\n",
    "        return None\n",
    "\n",
    "def preprocess_data(data):\n",
    "    # Drop irrelevant columns\n",
    "    data = data.drop(columns=['RowNumber', 'CustomerId', 'Surname'], axis=1)\n",
    "    \n",
    "    # Check for missing values\n",
    "    if data.isnull().any().any():\n",
    "        print(\"数据包含缺失值，请处理缺失值。\")\n",
    "        return None\n",
    "    \n",
    "    # Encode the categorical variables\n",
    "    label_encoder_gender = LabelEncoder()\n",
    "    data['Gender'] = label_encoder_gender.fit_transform(data['Gender'])\n",
    "    \n",
    "    onehot_encoder_geo = OneHotEncoder()\n",
    "    geo_encoded = onehot_encoder_geo.fit_transform(data[['Geography']])\n",
    "    geo_encoded_df = pd.DataFrame(geo_encoded.toarray(), columns=onehot_encoder_geo.get_feature_names_out(['Geography']))\n",
    "    \n",
    "    # Save the encoders\n",
    "    with open('label_encoder_gender.pkl', 'wb') as f:\n",
    "        pickle.dump(label_encoder_gender, f)\n",
    "    with open('onehot_encoder.pkl', 'wb') as f:\n",
    "        pickle.dump(onehot_encoder_geo, f)\n",
    "    \n",
    "    # Divide the data into features and target\n",
    "    X = pd.concat([data.drop(columns=['Exited', 'Geography'], axis=1), geo_encoded_df], axis=1)\n",
    "    y = data['Exited']\n",
    "    \n",
    "    return X, y\n",
    "\n",
    "def split_and_scale_data(X, y):\n",
    "    # Split the data into training and testing sets\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "    \n",
    "    # Scale the data\n",
    "    scaler = StandardScaler()\n",
    "    X_train = scaler.fit_transform(X_train)\n",
    "    X_test = scaler.transform(X_test)\n",
    "    \n",
    "    # Save the scaler\n",
    "    with open('scaler.pkl', 'wb') as f:\n",
    "        pickle.dump(scaler, f)\n",
    "    \n",
    "    return X_train, X_test, y_train, y_test\n",
    "\n",
    "# Load the dataset\n",
    "file_path = 'Churn_Modelling.csv'\n",
    "data = load_data(file_path)\n",
    "if data is not None:\n",
    "    # Preprocess the data\n",
    "    X, y = preprocess_data(data)\n",
    "    if X is not None and y is not None:\n",
    "        # Split and scale the data\n",
    "        X_train, X_test, y_train, y_test = split_and_scale_data(X, y)\n",
    "        print(\"数据预处理完成，训练集和测试集已准备好。\")\n",
    "        print(f\"训练集包含 {X_train.shape[0]} 个样本，测试集包含 {X_test.shape[0]} 个样本。\")\n",
    "        print(f\"特征维度为 {X_train.shape[1]}。\")\n",
    "        print(f\"目标变量的类别数为 {y.nunique()}。\")\n",
    "        print(f\"目标变量的类别分布为：\\n{y.value_counts()}\")\n",
    "        print(f\"训练集目标变量的类别分布为：\\n{y_train.value_counts()}\")\n",
    "        print(f\"测试集目标变量的类别分布为：\\n{y_test.value_counts()}\")\n",
    "        print(\"数据预处理完成。\")\n",
    "        print(\"数据已保存为 pickle 文件。\")\n",
    "        print(\"label_encoder_gender.pkl\")\n",
    "        print(\"onehot_encoder_geo.pkl\")\n",
    "        print(\"scaler.pkl\")\n",
    "        print(\"train_test_data.pkl\")\n",
    "\n",
    "## Build the ANN model\n",
    "def build_model(input_dim):\n",
    "    model = Sequential()\n",
    "    model.add(Dense(units=64, activation='relu', input_dim=input_dim)) ## Hidden layer 1 connected with input layer\n",
    "    model.add(Dense(units=32, activation='relu')) ## Hidden layer 2 connected with hidden layer 1\n",
    "    model.add(Dense(units=1, activation='sigmoid')) ## Output layer connected with hidden layer 2\n",
    "    \n",
    "    return model\n",
    "# build_model(X_train.shape[1]).summary()\n",
    "\n",
    "## compile the model\n",
    "def compile_model(model):\n",
    "    model.compile(optimizer=Adam(learning_rate=0.01), loss='binary_crossentropy', metrics=['accuracy'])\n",
    "    return model\n",
    "compile_model(build_model(X_train.shape[1])).summary()  \n",
    "\n",
    "## Train the model\n",
    "def train_model(model, X_train, y_train):\n",
    "    log_dir = \"logs/fit/\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "    tensorboard_callback = TensorBoard(log_dir=log_dir, histogram_freq=1)\n",
    "    ## Setup early stopping \n",
    "    early_stopping = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)  \n",
    "    model.fit(X_train, y_train, epochs=10, batch_size=32, validation_split=0.1, callbacks=[early_stopping, tensorboard_callback])\n",
    "    return model\n",
    "model = train_model(compile_model(build_model(X_train.shape[1])), X_train, y_train)\n",
    "model.save('churn_model.h5')\n",
    "\n",
    "## Evaluate the model\n",
    "def evaluate_model(model, X_test, y_test):\n",
    "    loss, accuracy = model.evaluate(X_test, y_test)\n",
    "    print(f\"Loss: {loss}\")\n",
    "    print(f\"Accuracy: {accuracy}\")\n",
    "evaluate_model(train_model(compile_model(build_model(X_train.shape[1])), X_train, y_train), X_test, y_test)\n",
    "\n",
    "## Load tensorboard extension\n",
    "%load_ext tensorboard\n",
    "%tensorboard --logdir logs/fit\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "markdown"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
